import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import ast
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset

# =========================
# Step 1: Define the Neural Network
# =========================
class RobotNN(nn.Module):
    def __init__(self, input_size, hidden_size=1024, output_size=4):  # Increase hidden size
        super(RobotNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.dropout1 = nn.Dropout(p=0.3)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.dropout2 = nn.Dropout(p=0.3)
        self.fc3 = nn.Linear(hidden_size, hidden_size)
        self.dropout3 = nn.Dropout(p=0.3)
        self.fc4 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout1(x)
        x = self.fc2(x)
        x = self.relu(x)
        x = self.dropout2(x)
        x = self.fc3(x)
        x = self.relu(x)
        x = self.dropout3(x)
        x = self.fc4(x)
        return x


# =========================
# Step 2: Load Training Data from CSV
# =========================
def load_training_data(csv_file, max_length=360):
    """
    Loads Lidar scan data and actions from a CSV file.
    The CSV is expected to have:
    - First column: JSON-like string of Lidar scan data (angle + distance pairs)
    - Last column: Action (0=Forward, 1=Left, 2=Right, 3=Stop)
    """
    df = pd.read_csv(csv_file, header=None)  # Load CSV without assuming headers

    # Parse the JSON-like strings in the first column
    lidar_data = df.iloc[:, 0].apply(ast.literal_eval)  # Convert strings to Python lists

    # Create a 360-item array for each Lidar scan
    def process_scan(scan):
        distances = [0] * max_length  # Initialize array with zeros
        for angle, distance in scan:
            rounded_angle = round(angle)  # Round angle to the nearest integer
            if 0 <= rounded_angle < max_length:  # Ensure angle is within bounds
                distances[rounded_angle] = distance
        return distances

    X = lidar_data.apply(process_scan).tolist()

    # Normalize distances (scale between -1 and 1)
    X = np.array(X)
    X = (X / np.max(X)) * 2 - 1

    # Extract the action labels (last column)
    y = df.iloc[:, 1].values

    # Split data into training and validation sets
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

    return torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long), torch.tensor(y_val, dtype=torch.long)


# =========================
# Step 3: Train the AI Model
# =========================
def train_model(csv_file, num_epochs=300, learning_rate=0.0001, batch_size=256):
    # Load data
    X_train, X_val, y_train, y_val = load_training_data(csv_file)
    input_size = X_train.shape[1]  # Dynamically determine input size

    # Create DataLoaders
    train_dataset = TensorDataset(X_train, y_train)
    val_dataset = TensorDataset(X_val, y_val)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, pin_memory=True)

    # Initialize model
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = RobotNN(input_size=input_size).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)

    # Early stopping parameters
    best_val_loss = float('inf')
    patience = 10  # Number of epochs to wait before stopping
    counter = 0

    # Training loop
    for epoch in range(num_epochs):
        model.train()
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()

        # Evaluate on validation set
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                val_outputs = model(X_batch)
                val_loss += criterion(val_outputs, y_batch).item()
        val_loss /= len(val_loader)  # Average validation loss
        if epoch % 10 == 0:  # Print every 10 epochs    
            print(f'Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}')

        # Early stopping logic
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            torch.save(model.state_dict(), "best_model.pth")  # Save the best model
        #else:
            #counter += 1
            #if counter >= patience:
             #   print("Early stopping triggered.")
              #  break

    print("Training complete!")

    # Save the trained model
    torch.save(model.state_dict(), "robot_model.pth")
    print("Model saved successfully!")


# =========================
# Step 4: Run Training
# =========================
csv_filename = "lidar_training_data.csv"  # Change this to your CSV file name
train_model(csv_filename)
